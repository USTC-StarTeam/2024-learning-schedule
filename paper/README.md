# 经典论文参考

## CV
> 计算机视觉领域的经典论文


**Survey**
1. [A Comprehensive Survey on Source-free Domain Adaptation](https://arxiv.org/abs/2302.11803)
2. [Domain Generalization in Computational Pathology: Survey and Guidelines](https://arxiv.org/abs/2310.19656)
3. [A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot](https://arxiv.org/abs/2307.14397)
4. [Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training](https://arxiv.org/abs/2305.13689)
5. [On the Design Fundamentals of Diffusion Models: A Survey](https://arxiv.org/abs/2306.04542)

**Paper**

图像分类

1. [AlexNet_ImageNet Classification with Deep Convolutional](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
2. [VGGNet_Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
3. [ResNet_Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
4. [DenseNet_Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
5. [ViT_An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

目标检测

6. [R-CNN_Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)
7. [Fast R-CNN](https://arxiv.org/abs/1504.08083)
8. [Mask R-CNN](https://arxiv.org/abs/1703.06870)
9. [YOLO_You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
10. [DETR_End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

语义分割

11. [BiSeNet_Bilateral Segmentation Network for Real-time Semantic Segmentation](https://arxiv.org/abs/1808.00897)
12. [FCN_Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038)
13. [OCRNet_Object-Contextual Representations for Semantic Segmentation](https://arxiv.org/abs/1909.11065)
14. [U-Net_Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
15. [Swin Transformer_Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

生成模型



##  NLP 

> 自然语言处理领域的经典论文

**Survey**

1. [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)
2. [Model-tuning Via Prompts Makes NLP Models Adversarially Robust](https://arxiv.org/abs/2303.07320)
3. [A Survey on In-context Learning](https://arxiv.org/abs/2301.00234)

**Paper**

传统自然语言处理

1. [Word2Vec_Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
2. [CNN_Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
3. [RNN_A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/abs/1506.00019)
4. [Seq2Seq_Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
5. [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)
6. [GloVe_Global Vectors for Word Representation](https://www-nlp.stanford.edu/pubs/glove.pdf)

大模型（LLM）

1. [Transformer_Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [GPT_Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
3. [GPT2_Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
4. [GPT3_Language Models are Few-Shot Learners]([2005.14165 (arxiv.org)](https://arxiv.org/pdf/2005.14165))
5. [GPT3.5_Training language models to follow instructions with human feedback]([2203.02155 (arxiv.org)](https://arxiv.org/pdf/2203.02155))
6. [GPT-4 Technical Report]([2303.08774 (arxiv.org)](https://arxiv.org/pdf/2303.08774))
7. [BERT_Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

大模型微调

1. [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODEL](https://arxiv.org/pdf/2106.09685)
2. 
3. [The Power of Scale for Parameter-Efficient Prompt Tuning]([[2104.08691\] The Power of Scale for Parameter-Efficient Prompt Tuning (arxiv.org)](https://arxiv.org/abs/2104.08691))
4. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models]([2201.11903 (arxiv.org)](https://arxiv.org/pdf/2201.11903))

多模态（Multi-model Language Modal）

1. [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/assets/palm-e.pdf)
2. [Visual Instruction Tuning]([2304.08485 (arxiv.org)](https://arxiv.org/pdf/2304.08485))
3. [TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation]([TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation (arxiv.org)](https://arxiv.org/pdf/2305.00447))
4. [LLaRA: Aligning Large Language Models with Sequential Recommenders](https://arxiv.org/pdf/2312.02445v2)



## RS

> 推荐系统领域的经典论文

**Survey**

1. [A Survey on User Behavior Modeling in Recommender Systems](https://arxiv.org/abs/2302.11087)
2. [Disentangled Representation Learning](https://arxiv.org/abs/2211.11695)
3. [A Cookbook of Self-Supervised Learning](https://arxiv.org/abs/2304.12210)
4. [ Self-Supervised Learning for Recommender Systems: A Survey](https://arxiv.org/abs/2203.15876)
5. [Graph Neural Networks in Recommender Systems: A Survey](https://arxiv.org/abs/2011.02260)

**Paper**

1. [Bayesian Personalized Ranking](https://zhuanlan.zhihu.com/p/25069367)
2. [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031)
3. [Neural Graph Collaborative Filtering](https://arxiv.org/abs/1905.08108)
4. [LightGCN_Simplifying and Powering Graph Convolution Network for Recommendation](https://arxiv.org/abs/2002.02126)
5. [Self-Attentive Sequential Recommendation](https://arxiv.org/abs/1808.09781)
6. [Intent-aware Ranking Ensemble for Personalized Recommendation](https://arxiv.org/abs/2304.07450)
7. [LightGT_A Light Graph Transformer for Multimedia Recommendation](https://dl.acm.org/doi/10.1145/3539618.3591716)
8. [Graph Transformer for Recommendation](https://arxiv.org/abs/2306.02330)
9. [Learning Disentangled Representations for Recommendation](https://arxiv.org/abs/1910.14238)
10. [Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/abs/1706.06978)
11. [Less is More: Reweighting Important Spectral Graph Features for Recommendation](https://arxiv.org/abs/2204.11346)
12. [Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction](https://arxiv.org/abs/2006.05639)
13. [Multi-behavior Self-supervised Learning for Recommendation](https://arxiv.org/abs/2305.18238)
14. [Multi-Scenario Ranking with Adaptive Feature Learning](https://arxiv.org/abs/2306.16732)
15. [Towards Multi-Interest Pre-training with Sparse Capsule Network](https://dl.acm.org/doi/abs/10.1145/3539618.3591778)



## Re

